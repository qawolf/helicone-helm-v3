################################################################################
#
#                     HELICONE INFRASTRUCTURE
#
################################################################################

# Cluster Autoscaler configuration
clusterAutoscaler:
  enabled: true
  image:
    tag: "v1.30.0"
  clusterName: "helicone"
  serviceAccount:
    create: true
    name: "cluster-autoscaler"
    # Note: No IAM role annotations needed - EKS Pod Identity handles this automatically
    # The role association is managed by Terraform via aws_eks_pod_identity_association
  extraArgs:
    - "--scale-down-delay-after-add=10m"
    - "--scale-down-unneeded-time=10m"
    - "--max-node-provision-time=15m"
    - "--scan-interval=10s"

  # OpenTelemetry configuration
  otel:
    endpoint: "http://otel-collector.helicone-infrastructure.svc.cluster.local:4317"
    headers: ""
    protocol: "grpc"

# Nginx Ingress Controller configuration
nginxIngressController:
  enabled: true
  namespace: helicone-infrastructure
  fullnameOverride: "nginx-controller"

  # Set to false if you have an existing nginx-ingress-controller that conflicts
  # This will skip creating nginx resources to avoid ownership conflicts
  skipExistingResources: false

  # Watch all namespaces for ingress resources
  watchIngressWithoutClass: true
  watchNamespaces: "" # Empty string means watch all namespaces

  # Controller configuration
  controller:
    image:
      repository: registry.k8s.io/ingress-nginx/controller
      tag: "v1.8.2"
      pullPolicy: IfNotPresent

    # Enable cross-namespace ingress support
    extraArgs:
      - "--watch-ingress-without-class=true"
      - "--ingress-class=nginx"
      - "--enable-ssl-passthrough"

    # Resource configuration
    resources:
      requests:
        cpu: 100m
        memory: 90Mi
      limits:
        cpu: 500m
        memory: 512Mi

    # Service configuration - Using Pod Identity with proper permissions
    service:
      type: LoadBalancer # Back to LoadBalancer - Pod Identity should fix permissions
      annotations:
        service.beta.kubernetes.io/aws-load-balancer-type: nlb
        service.beta.kubernetes.io/aws-load-balancer-scheme: internet-facing # Make NLB internet-facing
        service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
        # Pod Identity handles authentication automatically - no more manual annotations needed
      ports:
        http: 80
        https: 443

    # Ingress class configuration
    ingressClass:
      name: nginx
      enabled: true
      default: true # Keep nginx as default for existing ingresses
      controllerValue: "k8s.io/ingress-nginx"

    # RBAC configuration for cross-namespace access
    rbac:
      create: true
      scope: true # Cluster-wide scope for all namespaces

    # Service account - IMPORTANT: Must match Terraform Pod Identity association
    serviceAccount:
      create: true
      name: nginx-ingress-controller # This matches the Pod Identity association

# Prometheus configuration
prometheus:
  enabled: true
  # When prometheus is enabled, these subcomponents are configured
  image:
    tag: "v3.4.1"
  alertmanager:
    enabled: false
  kube-state-metrics:
    enabled: false
  prometheus-node-exporter:
    enabled: false
  prometheus-pushgateway:
    enabled: false

  global:
    ## How frequently to scrape targets by default
    scrape_interval: 15s
    scrape_timeout: 10s
    ## How frequently to evaluate rules
    evaluation_interval: 15s

  resources:
    limits:
      cpu: 1000m
      memory: 512Mi
    requests:
      cpu: 500m
      memory: 512Mi

  # https://github.com/prometheus-community/helm-charts/blob/ec4f325616989d93c204012c57199e98e84b8c87/charts/prometheus/values.yaml#L702
  hostNetwork: true

  server:
    fullnameOverride: "prometheus"
    replicaCount: 1
    persistentVolume:
      mountPath: "/var/lib/prometheus"
    extraFlags:
      - web.enable-lifecycle
      - web.enable-remote-write-receiver
      - web.enable-otlp-receiver
      - enable-feature=exemplar-storage

# Tempo configuration for distributed tracing
tempo:
  enabled: true
  fullnameOverride: "tempo"
  persistence:
    enabled: true
    size: "10Gi"
  serviceMonitor:
    enabled: true
  retention: "7d"

  # Enable distributed mode
  traces:
    otlp:
      grpc:
        enabled: true
      http:
        enabled: true

  # Configure the distributor to receive OTLP
  distributor:
    config:
      receivers:
        otlp:
          protocols:
            grpc:
              endpoint: 0.0.0.0:4317
            http:
              endpoint: 0.0.0.0:4318

# Loki configuration
# Loki configuration for log aggregation
loki:
  enabled: false
  fullnameOverride: "loki"
  singleBinary:
    replicas: 1
  image:
    repository: grafana/loki
    tag: "3.5.0"
    pullPolicy: IfNotPresent

  # Resource configuration
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 500m
      memory: 1Gi

  # Service configuration
  service:
    type: ClusterIP
    ports:
      http:
        port: 3100
        targetPort: 3100

  # Loki configuration
  config:
    auth_enabled: false
    server:
      http_listen_port: 3100
      grpc_listen_port: 9096
    common:
      path_prefix: /loki
      storage:
        filesystem:
          chunks_directory: /loki/chunks
          rules_directory: /loki/rules
      replication_factor: 1
      ring:
        instance_addr: 127.0.0.1
        kvstore:
          store: inmemory
    query_scheduler:
      max_outstanding_requests_per_tenant: 32768
    schema_config:
      configs:
        - from: 2020-05-15
          store: tsdb
          object_store: filesystem
          schema: v13
          index:
            prefix: index_
            period: 24h
    ruler:
      alertmanager_url: http://localhost:9093
    limits_config:
      reject_old_samples: true
      reject_old_samples_max_age: 168h
      allow_structured_metadata: true
    analytics:
      reporting_enabled: false
  # Zero out replica counts of other deployment modes
  backend:
    replicas: 0
  read:
    replicas: 0
  write:
    replicas: 0

  ingester:
    replicas: 0
  querier:
    replicas: 0
  queryFrontend:
    replicas: 0
  queryScheduler:
    replicas: 0
  distributor:
    replicas: 0
  compactor:
    replicas: 0
  indexGateway:
    replicas: 0
  bloomCompactor:
    replicas: 0
  bloomGateway:
    replicas: 0

# OpenTelemetry Collector subchart values
opentelemetry-collector:
  enabled: true
  mode: deployment
  replicaCount: 1
  command:
    name: "otelcol"
  image:
    repository: "otel/opentelemetry-collector"
    tag: "0.127.0"

  fullnameOverride: "otel-collector"

  # Override the default config with your custom configuration
  config:
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317

    exporters:
      otlp:
        endpoint: tempo.helicone-infrastructure.svc.cluster.local:4317
        tls:
          insecure: true
      # otlphttp:
      #   endpoint: http://loki.helicone-infrastructure.svc.cluster.local:3100/otlp
      #   tls:
      #     insecure: true
      prometheusremotewrite:
        endpoint: http://prometheus.helicone-infrastructure.svc.cluster.local:80/api/v1/write
        tls:
          insecure: true

    extensions:
      health_check:
        endpoint: ${env:MY_POD_IP}:13133

    processors:
      batch:
        timeout: 1s
        send_batch_size: 1024
      memory_limiter:
        check_interval: 5s
        limit_mib: 500
        spike_limit_mib: 100

    service:
      extensions:
        - health_check
      pipelines:
        traces:
          receivers:
            - otlp
          processors:
            - batch
          exporters:
            - otlp
        metrics:
          receivers:
            - otlp
          processors:
            - batch
          exporters:
            - prometheusremotewrite
        # logs:
        #   receivers:
        #     - otlp
        #   processors:
        #     - batch
        #   exporters:
        #     - otlphttp

  # Resource configuration
  resources:
    limits:
      cpu: 500m
      memory: 1024Mi
    requests:
      cpu: 250m
      memory: 256Mi

  # Enable GOMEMLIMIT based on memory limits
  useGOMEMLIMIT: true

  # Service configuration
  service:
    type: ClusterIP

  # Only expose the OTLP ports
  ports:
    otlp:
      enabled: true
      containerPort: 4317
      servicePort: 4317
      protocol: TCP
      appProtocol: grpc
    otlp-http:
      enabled: false
    jaeger-compact:
      enabled: false
    jaeger-thrift:
      enabled: false
    jaeger-grpc:
      enabled: false
    zipkin:
      enabled: false
    metrics:
      enabled: false

# AWS Load Balancer Controller configuration
# TODO DRY up between awsLoadBalancerController and aws-load-balancer-controller
awsLoadBalancerController:
  enabled: true

# AWS Load Balancer Controller subchart configuration
aws-load-balancer-controller:
  # Cluster name is required for the controller to work
  clusterName: "helicone" # This should match your EKS cluster name

  # Service account configuration - MANAGED BY TERRAFORM
  # Pod Identity association is managed by Terraform (terraform/eks/addons.tf)
  # ServiceAccount creation and IAM role association handled automatically
  serviceAccount:
    create: true
    name: "aws-load-balancer-controller"
    # No annotations needed for Pod Identity - association managed by Terraform

  # Controller configuration
  replicaCount: 2

  # Resources
  resources:
    limits:
      cpu: 200m
      memory: 500Mi
    requests:
      cpu: 100m
      memory: 200Mi

  # Enable webhook for cert-manager integration
  enableCertManager: false

  # Region configuration (auto-detected from IMDS if not set)
  region: ""

  # VPC ID (auto-detected if not set)
  vpcId: ""

  # Additional controller arguments
  additionalResourceTags: {}

  # Default ingress class
  ingressClass: alb
  createIngressClassResource: true

  # Enable shield, WAF, and WAFv2 (optional)
  enableShield: false
  enableWaf: false
  enableWafv2: false

  # Log level
  logLevel: info

  # Pod disruption budget
  podDisruptionBudget:
    maxUnavailable: 1

  # Update strategy
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1

  # Node selector for pod assignment
  nodeSelector: {}

  # Tolerations for pod assignment
  tolerations: []

  # Affinity for pod assignment
  affinity: {}

  # Priority class name
  priorityClassName: ""

  # Security context
  securityContext:
    fsGroup: 65534

  # Pod labels
  podLabels: {}

  # Pod annotations
  podAnnotations: {}

  # Environment variables
  env: []

  # Volume mounts
  volumeMounts: []

  # Volumes
  volumes: []

  # Liveness probe configuration
  livenessProbe:
    failureThreshold: 2
    httpGet:
      path: /healthz
      port: 61779
      scheme: HTTP
    initialDelaySeconds: 30
    timeoutSeconds: 10

  # Namespace to watch (empty = all namespaces)
  watchNamespace: ""

  # Default target type for ALBs (instance or ip)
  defaultTargetType: instance

  # Enable pod readiness gate inject
  enablePodReadinessGateInject: true

  # Enable endpoint slices
  enableEndpointSlices: true

  # Service monitor for Prometheus
  serviceMonitor:
    enabled: false
    additionalLabels: {}
    interval: 30s
    namespace: ""
